{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning tweets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxwo4O+7JAq4etYbS25wbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed-Shatla/NLP-data-cleaning/blob/main/Cleaning_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wGMGAOCcbHng"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#downloading"
      ],
      "metadata": {
        "id": "6hl4zRz3cP6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "id": "IERzFriVbbUi",
        "outputId": "f18ace0b-5f8e-47e9-94ac-a09826610e09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading data"
      ],
      "metadata": {
        "id": "dnLKwE-XcgFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "ldPQy3sfbziE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data cleaning"
      ],
      "metadata": {
        "id": "yk_7zaS9cjb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "GX8y_9WjcjCW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "  text= re.sub(r'#[\\S|_]*','',text) #hashtage removing\n",
        "  text= re.sub(r'@[\\S|_]*','',text) #username removing\n",
        "  text= re.sub(r'https?:\\/\\/\\S+','',text) #hyperlink removing\n",
        "  text = re.sub(r'\\W',' ',text) #remove any emotions\n",
        "  text = re.sub(r'\\d+','',text) #remove any standalone digits\n",
        "  text = re.sub(r'^\\s+','',text)#remove spaces that at start of the sentences\n",
        "  text = re.sub(r'\\s+$','',text)#remove spaces that at end of the sentences\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "0rkSHwkhcLCX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanText(pos_tweets[0])"
      ],
      "metadata": {
        "id": "Q8-VXq75fe04",
        "outputId": "9d9ad7d7-213a-420d-a648-5cedbba833c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for being top engaged members in my community this week'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "xHTassmjjY1h",
        "outputId": "a87fe2d6-d5e3-42b6-cb6c-2faf3f8d9308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "def process_on_tweets(tweets):\n",
        "  result=[]\n",
        "  for tweet in tweets:\n",
        "    tweet = cleanText(tweet)\n",
        "    tweet = tweet.split()\n",
        "    tweet=[word for word in tweet if word.lower() not in stop_words]\n",
        "\n",
        "    ps = PorterStemmer()\n",
        "    tweet=[ps.stem(word) for word in tweet]\n",
        "\n",
        "    result.append(tweet)\n",
        "  return result"
      ],
      "metadata": {
        "id": "E-fXmeTsflF1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = process_on_tweets(pos_tweets)\n",
        "negative_tweets = process_on_tweets(neg_tweets)"
      ],
      "metadata": {
        "id": "B1Vtcm_WyVA5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Frequinces\n",
        "### (word,label):freq"
      ],
      "metadata": {
        "id": "7SDkY0Dxy8Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freq(tweets,label):\n",
        "  freq ={}\n",
        "  for tweet in tweets:\n",
        "    for word in tweet:\n",
        "      key = (word,label)\n",
        "      if key in freq:\n",
        "        freq[key]+=1\n",
        "      else:\n",
        "        freq[key]=1\n",
        "\n",
        "  return freq"
      ],
      "metadata": {
        "id": "d64G3WST1zDE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_freq = build_freq(positive_tweets,1)\n",
        "neg_freq = build_freq(negative_tweets,0)"
      ],
      "metadata": {
        "id": "RZkIDVGR3ZV7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#building features"
      ],
      "metadata": {
        "id": "T66wR2rF4dnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_features(tweets,p_freq,n_freq):\n",
        "  features=[]\n",
        "  temp_vec = []\n",
        "\n",
        "  for tweet in tweets:\n",
        "    temp_vec=[0]*3   #[0,0,0]\n",
        "    p_count = 0\n",
        "    n_count = 0\n",
        "    temp_vec[0]=1   #bias = 1\n",
        "    for word in tweet:\n",
        "      p_count+=p_freq.get((word,1),1)\n",
        "      n_count+=n_freq.get((word,0),0)\n",
        "\n",
        "    temp_vec[1]=p_count\n",
        "    temp_vec[2]=n_count\n",
        "\n",
        "\n",
        "    features.append(temp_vec)\n",
        "  return features"
      ],
      "metadata": {
        "id": "yXgsM1A64Xpr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_features = build_features(positive_tweets,pos_freq,neg_freq)\n",
        "neg_features = build_features(negative_tweets,pos_freq,neg_freq)"
      ],
      "metadata": {
        "id": "jQEpJrNs9lTQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_features[0]"
      ],
      "metadata": {
        "id": "a2EzIr1W92se",
        "outputId": "d1da1cec-1b06-4a41-ca86-0f4560065ec8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 173, 73]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pos_features)"
      ],
      "metadata": {
        "id": "lBW9YBTcFp0d",
        "outputId": "520b01f7-d023-41b9-cb39-d4ec0614ecb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pos_y = np.ones((len(pos_features),1))\n",
        "neg_y = np.zeros((len(neg_features),1))"
      ],
      "metadata": {
        "id": "w13OkiDU940x"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=[]\n",
        "y = []\n",
        "for i in range(len(pos_features)):\n",
        "  X.append(pos_features[i])\n",
        "  X.append(neg_features[i])\n",
        "  y.append(pos_y[i])\n",
        "  y.append(neg_y[i])\n"
      ],
      "metadata": {
        "id": "E7BugwdZBQ89"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}